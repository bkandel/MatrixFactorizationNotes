\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\newcommand{\transpose}{^\mathrm{T}}
\title{Dimensionality Reduction Algorithms for Medical Imaging: PCA, NMF, ICA, etc.}
\author{Ben Kandel}
\begin{document}
\maketitle
\section*{Introduction}
Dimensionality reduction algorithms can be written as matrix factorization problems.  The basic form of the factorization is something like 
\begin{equation}
\mathbf{X} \approx \mathbf{WV}\transpose
\label{eqn:base}
\end{equation}
Books and articles have different notations for the dimensionality of $\mathbf{X}$ and the different matrices (so sometimes you will see that $\mathbf{X}$ is the data matrix, and sometimes $\mathbf{X}^{\mathrm{T}}$), and the transposes are propagated along all the steps of the decomposition.  We'll try to be consistent and use the following notation: 
\begin{itemize}
\item The data matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ has $n$ rows, with each row corresponding to an observation (one subject), and $p$ columns, with each column correponding to a variable (voxel). 
\item The loading matrix $\mathbf{W} \in \mathbb{R}^{n \times r}$ corresponds to the reduced-dimensionality version of $\mathbf{X}$, replacing the $p$ variables with $r \ll p$ variables.
\item The basis matrix  $\mathbf{V} \in \mathbb{R}^{p \times r}$ corresponds to the eigenvector matrix from classical PCA. 
\end{itemize}
Using this notation, the loading matrix $\mathbf{W}$ corresponds to the projections of the data matrix on the basis matrix, $\mathbf{W} = \mathbf{XV}$.  
 
\section*{PCA}
The most widely used dimensionality reduction algorithm is PCA.  PCA finds an  orthogonal rotation of the covariance matrix $\mathbf{X}^{\mathrm{T}}\mathbf{X}$ that satisfies one of the following problems:
\begin{enumerate}
\item Maximize variance in projected space: 
\begin{equation}
\begin{aligned}
& \underset{\mathbf{V}}{\text{maximize}} & &\| \mathbf{X} \mathbf{V} \|_2^2 \\ 
&\text{subject to} & &\mathbf{V}^{\mathrm{T}} \mathbf{V} = \mathbf{I}
\end{aligned}
\end{equation}
\item Minimizing reconstruction error:
\begin{equation}
\begin{aligned}
& \underset{\mathbf{V}}{\text{minimize}} & &  \| \mathbf{X} - \mathbf{VV}^{\mathrm{T}}  \mathbf{X}\|_2^2 \\ 
&\text{subject to} & & \mathbf{V}^{\mathrm{T}} \mathbf{V}= \mathbf{I}
\end{aligned}
\label{eqn:reconstruction_error}
\end{equation}
\end{enumerate} 
In either case, orthogonality in the projected space is enforced.
 
\subsection*{Sparse PCA}
In sparse PCA, additional constraints are enforced on the eigenvectors of the covariance matrix.  This gives us something like 
\begin{equation}
\underset{\mathbf{V}}{\text{minimize}} \| \mathbf{X} - \mathbf{VV}^{\mathrm{T}}\mathbf{X}\|_2^2  + \lambda \sum_i \vert V_i \vert,
\end{equation}
where each column in $\mathbf{V}$ is $V_i$. Enforcing sparsity, though, normally entails discarding orthogonality.  Because the ``eigenvectors'' are not orthogonal, one component of the data matrix may project onto more than one ``eigenvector,'' so total variance explained is not an appropriate measure of how good an approximation to the original matrix the sparse eigenvectors are.   Most versions of SPCA still do have some sort of constraints, though: 
\begin{enumerate}
\item Changing notation to standard SVD notation ($\mathbf{X} = \mathbf{UDV}^{\mathrm{T}}$), \cite{witten_penalized_2009} enforces $\|\mathbf{u}\|_2^2 \leq 1, \, \|\mathbf{v}\|_2^2 \leq 1$ (but not orthogonality). This is sort of a lower bound on the kinds of restraints you have--each vector must at the very least not increase the size of the original matrix. 
\item Jolliffe \cite{jolliffe_simplified_2000} \textit{does} enforce orthogonality on the eigenvectors.  The tradeoff this presents is that the output is not uncorrelated: $\mathbf{V}^{\mathrm{T}} \mathbf{V} = \mathbf{I}$, but $\mathbf{V}\transpose\mathbf{X}^{\mathrm{T}} \mathbf{XV}$ is not diagonal. 
\item Zou \cite{zou_sparse_2006} splits the eigenvector matrices ($\mathbf{VV}\transpose$) in two, $\mathbf{AB}\transpose$.  He then enforces orthogonality on $\mathbf{A}$ and sparsity on $\mathbf{B}$: 
\begin{equation}
\underset{\mathbf{A}, \mathbf{B}}{\text{minimize}} \| \mathbf{X} - \mathbf{A}\mathbf{B}\transpose \mathbf{X}\|_2^2 + p\left(\mathbf{B}\right),
\end{equation}
where $p\left(\cdot\right)$ is the sparsity penalty and $\mathbf{A}\transpose\mathbf{A} = \mathbf{I}$. 
\item Shen \cite{shen_sparse_2008} only really gives results for a rank-one approximation of the data matrix.  He basically iterates between projecting on $\mathbf{u}$ and $\mathbf{v}$, applying a thresholding operation, and re-scaling so that the vectors have unit norm.  Except for the re-scaling (so that $\|\mathbf{v}\|_2^2 = 1$), there is no other constraint.
\end{enumerate}

\section*{NMF}
NMF has two objective functions (see ``Algorithms for Non-Negative Matrix Factorization'' in NIPS 2001):
\begin{enumerate}
\item \begin{equation}
\begin{aligned}
&\underset{\mathbf{V}, \mathbf{W}}{\text{minimize}} &&\| \mathbf{X} - \mathbf{VW}\transpose\|_2^2 \\ 
& \text{subject to} & &\mathbf{V,W} \succeq 0 
\end{aligned}
\end{equation}
\item
\begin{equation}
\begin{aligned}
&\underset{\mathbf{V}, \mathbf{W}}{\text{minimize}} && D\left(\mathbf{X} \|  \mathbf{VW}\transpose\right) \\ 
& \text{subject to} & &\mathbf{V,W} \succeq 0, 
\end{aligned}
\end{equation}
where $D\left(X\|Y\right)$ is the divergence (KL, if they're probabilities) of the two arguments.
\end{enumerate}
There are no orthogonality, diagonalization, etc.\ constraints at all. 

\section*{ICA}
ICA uses a fundamentally different objective function from PCA. As explained before,  PCA maximizes the variance of the projections onto the eigenvectors.  This objective has a simple closed-form solution, but suffers from the drawback of assuming that the variance of the projections is a sufficient statistic to characterize the data.  If the data follows a Gaussian distribution, the variance is a sufficient statistic.  ICA, on the other hand, does not assume Gausssianity of the data.  Because of this, it cannot use variance as a statistic.  Instead, it first whitens the data so that it has unit variance, and then computes ``eigenvectors'' that give independent projections. 

Instead of using the variance of the projections, ICA (at least as implemented in the FastICA algorithm, which is the most widely used algorithm for ICA \cite{hyvarinen_fast_1999,hyvarinen_independent_2000}) uses the negentropy, which is a measure of how far apart a given random variable is from Gaussianity: 
\begin{equation}
J_G \left( \mathbf{v}\right) = \left[ E \left\lbrace G \left( \mathbf{v}\transpose \mathbf{x}\right) \right\rbrace - E \left\lbrace G \left( \nu \right) \right\rbrace \right]^2,
\end{equation}
where $\mathbf{v}$ is a column of $\mathbf{V}$ and $\mathbf{x}$ is a row of $\mathbf{X}$,  $\nu$ is a Gaussian rv with unit variance (same as $\mathbf{v}$) and $G$ is some non-quadratic  ``contrast'' function, such as $\log \cosh (x)$. One other major difference between PCA and ICA is that the orthogonality constraint is replaced by a uncorrelatedness constraint.\footnote{Just to be clear: The difference between orthogonal and uncorrelated is that uncorrelated utilizes to the dot product of \textit{centered} vectors, and orthogonal uses the dot product of \textit{uncentered} vectors.  See \cite{rodgers_linearly_1984}.}  With these two modifications, we can write the objective function of ICA as follows:

 \begin{equation}
\begin{aligned}
&\underset{\mathbf{v}}{\text{maximize}} && \sum_{i=1}^n J_G \left( \mathbf{v_i} \right) \\ 
& \text{subject to} & & E \left\lbrace \left( \mathbf{v}_i\transpose \mathbf{x} \right) \left( \mathbf{v}_j\transpose \mathbf{x} \right) \right\rbrace = \delta_{ij}
\end{aligned}
\end{equation}
The FastICA optimization procedure involves Gram--Schmidt ``decorrelation'' (which here seems to me the same as orthogonalization, unless I'm missing something), the same as in power method iteration versions of PCA.  So I'm not sure how different these really are in practice.

\bibliographystyle{plain}
\bibliography{kandel_lib}

\end{document}