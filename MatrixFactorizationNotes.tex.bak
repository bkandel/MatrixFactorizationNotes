\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{url}
\usepackage{booktabs}
\newcommand{\transpose}{^\mathrm{T}}
\title{Dimensionality Reduction Algorithms for Medical Imaging: PCA, NMF, ICA, etc.}
\author{Ben Kandel}
\begin{document}
\maketitle
\section*{Introduction}
Dimensionality reduction algorithms can be written as matrix factorization problems.  The basic form of the factorization is something like 
\begin{equation}
\mathbf{X} \approx \mathbf{WV}\transpose
\label{eqn:base}
\end{equation}
Books and articles have different notations for the dimensionality of $\mathbf{X}$ and the different matrices (so sometimes you will see that $\mathbf{X}$ is the data matrix, and sometimes $\mathbf{X}^{\mathrm{T}}$), and the transposes are propagated along all the steps of the decomposition.  We'll try to be consistent and use the following notation: 
\begin{itemize}
\item The data matrix $\mathbf{X} \in \mathbb{R}^{n \times p}$ has $n$ rows, with each row corresponding to an observation (one subject), and $p$ columns, with each column correponding to a variable (voxel). 
\item The loading matrix $\mathbf{W} \in \mathbb{R}^{n \times r}$ corresponds to the reduced-dimensionality version of $\mathbf{X}$, replacing the $p$ variables with $r \ll p$ variables.
\item The basis matrix  $\mathbf{V} \in \mathbb{R}^{p \times r}$ corresponds to the eigenvector matrix from classical PCA. 
\end{itemize}
Using this notation, the loading matrix $\mathbf{W}$ corresponds to the projections of the data matrix on the basis matrix, $\mathbf{W} = \mathbf{XV}$.  
 
\section*{PCA}
The most widely used dimensionality reduction algorithm is PCA.  PCA finds an  orthogonal rotation of the covariance matrix $\mathbf{X}^{\mathrm{T}}\mathbf{X}$ that satisfies one of the following problems:
\begin{enumerate}
\item Maximize variance in projected space: 
\begin{equation}
\begin{aligned}
& \underset{\mathbf{V}}{\text{maximize}} & &\| \mathbf{X} \mathbf{V} \|_2^2 \\ 
&\text{subject to} & &\mathbf{V}^{\mathrm{T}} \mathbf{V} = \mathbf{I}
\end{aligned}
\end{equation}
\item Minimizing reconstruction error:
\begin{equation}
\begin{aligned}
& \underset{\mathbf{V}}{\text{minimize}} & &  \| \mathbf{X} - \mathbf{VV}^{\mathrm{T}}  \mathbf{X}\|_2^2 \\ 
&\text{subject to} & & \mathbf{V}^{\mathrm{T}} \mathbf{V}= \mathbf{I}
\end{aligned}
\label{eqn:reconstruction_error}
\end{equation}
\end{enumerate} 
In either case, orthogonality in the projected space is enforced.
 
\subsection*{Sparse PCA}
In sparse PCA, additional constraints are enforced on the eigenvectors of the covariance matrix.  This gives us something like 
\begin{equation}
\underset{\mathbf{V}}{\text{minimize}} \| \mathbf{X} - \mathbf{VV}^{\mathrm{T}}\mathbf{X}\|_2^2  + \lambda \sum_i \vert V_i \vert,
\end{equation}
where each column in $\mathbf{V}$ is $V_i$. Enforcing sparsity, though, normally entails discarding orthogonality.  Because the ``eigenvectors'' are not orthogonal, one component of the data matrix may project onto more than one ``eigenvector,'' so total variance explained is not an appropriate measure of how good an approximation to the original matrix the sparse eigenvectors are.  There are several different ways of dealing with this issue.  One way that seems reasonable to me is that proposed by Shen \cite{shen_sparse_2008}, which computes the ``adjusted variance explained'' as the difference between the projections of the data matrix projected on the column space of the rank-$k$ and the rank-($k$-1) sparse approximations of the data matrix.  This variance explained is then normalized by the norm of the data matrix so that it scales from 0 to 1 to obtain the percentage of explained variance.  In equations, the adjusted variance explained of the $k$'th component is given by $\text{tr} \left( \mathbf{X}_k^{\mathrm{T}} \mathbf{X}_k\right)  - \text{tr}\left(\mathbf{X}_{k-1}^{\mathrm{T}}\mathbf{X}_{k-1}\right)$, with $\mathbf{X}_k=\mathbf{XV}_k\left(\mathbf{V}_k^\mathrm{T}\mathbf{V}_k\right)^{-1}\mathbf{V}_k^\mathrm{T}$ and $\mathbf{V}_k$ the first $k$ vectors of the sparse basis matrix. 

 Most versions of SPCA still do have some sort of constraints, though: 
\begin{enumerate}
\item Changing notation to standard SVD notation ($\mathbf{X} = \mathbf{UDV}^{\mathrm{T}}$), \cite{witten_penalized_2009} enforces $\|\mathbf{u}\|_2^2 \leq 1, \, \|\mathbf{v}\|_2^2 \leq 1$ (but not orthogonality). This is sort of a lower bound on the kinds of restraints you have--each vector must at the very least not increase the size (i.e., $\ell_2$ norm) of the original matrix. 
\item Jolliffe \cite{jolliffe_simplified_2000} \textit{does} enforce orthogonality on the eigenvectors.  The tradeoff this presents is that the output is not uncorrelated: $\mathbf{V}^{\mathrm{T}} \mathbf{V} = \mathbf{I}$, but $\mathbf{V}\transpose\mathbf{X}^{\mathrm{T}} \mathbf{XV}$ is not diagonal.  Although Jolliffe does not explicitly address why he chooses orthogonality over uncorrelatedness, it seems that most authors who choose to maximize variance enforce orthogonality.  The reason this seems to be the case is that without a reconstruction error term, maximizing variance without orthogonality may just return the best rank-1 approximation of the data matrix over and over again.  When incorporating reconstruction error into the decomposition, incorporation of orthogonality is not as necessary (\url{http://ai.stanford.edu/~quocle/LeKarpenkoNgiamNg.pdf}).  
\item Zou \cite{zou_sparse_2006} splits the eigenvector matrices ($\mathbf{VV}\transpose$) in two, $\mathbf{AB}\transpose$.  He then enforces orthogonality on $\mathbf{A}$ and sparsity on $\mathbf{B}$: 
\begin{equation}
\underset{\mathbf{A}, \mathbf{B}}{\text{minimize}} \| \mathbf{X} - \mathbf{A}\mathbf{B}\transpose \mathbf{X}\|_2^2 + p\left(\mathbf{B}\right),
\end{equation}
where $p\left(\cdot\right)$ is the sparsity penalty and $\mathbf{A}\transpose\mathbf{A} = \mathbf{I}$. 
\item Shen \cite{shen_sparse_2008} only really gives results for a rank-one approximation of the data matrix.  He basically iterates between projecting on $\mathbf{u}$ and $\mathbf{v}$, applying a thresholding operation, and re-scaling so that the vectors have unit norm.  Except for the re-scaling (so that $\|\mathbf{v}\|_2^2 = 1$), there is no other constraint.
\end{enumerate}

\section*{NMF}
NMF has two objective functions (see ``Algorithms for Non-Negative Matrix Factorization'' in NIPS 2001):
\begin{enumerate}
\item \begin{equation}
\begin{aligned}
&\underset{\mathbf{V}, \mathbf{W}}{\text{minimize}} &&\| \mathbf{X} - \mathbf{VW}\transpose\|_2^2 \\ 
& \text{subject to} & &\mathbf{V,W} \succeq 0 
\end{aligned}
\end{equation}
\item
\begin{equation}
\begin{aligned}
&\underset{\mathbf{V}, \mathbf{W}}{\text{minimize}} && D\left(\mathbf{X} \|  \mathbf{VW}\transpose\right) \\ 
& \text{subject to} & &\mathbf{V,W} \succeq 0, 
\end{aligned}
\end{equation}
where $D\left(X\|Y\right)$ is the divergence (KL, if they're probabilities) of the two arguments, having the form $D\left(A\Vert B\right)=\sum_{ij}\left(A_{ij}\log\frac{A_{ij}}{B_{ij}} - A_{ij} + B_{ij}\right)$.  No motivation is given for the introduction of the divergence metric as opposed to Frobenius norms.  It's unclear to me what benefit it offers. 
\end{enumerate}
There are no orthogonality, diagonalization, etc.\ constraints at all in the original formulation.  Since the original formulation, there have been a few additions that included different constraints. Li \cite{li_learning_2001} includes an orthonormality constraint.  He includes a soft penalty rather than a hard constraint, so that his objective function is of the form 
\begin{equation}
\begin{aligned}
&\underset{\mathbf{V}, \mathbf{W}}{\text{minimize}} &&\| \mathbf{X} - \mathbf{VW}\transpose\|_2^2 + \lambda \sum_i \sum_{j \neq i} v_{ij}\\  
& \text{subject to} & &\mathbf{V,W} \succeq 0 
\end{aligned}
\end{equation}
It seems to me that the reason that orthogonality is not necessary for NMF when reconstrution cost is used is that when non-negativity constraints are used, reconstruction costs take the place of orthogonality (like what we saw before for PCA, but even stronger, because there can't be cancellation). (This should probably be formalized.)  

\section*{ICA}
As opposed to PCA and variants, which aim to perform dimensionality reduction, ICA (independent components analysis) was originally proposed to solve the ``blind source separation'' problem in which the observed signals are a mixture of the source signals, which are statistically independent.  Because of this focus of the problem, the nomenclature is different in ICA.  The standard ICA nomenclature is as follows: 
\begin{equation}
\mathbf{X} = \mathbf{As},
\end{equation}
where $\mathbf{X}$ is the observed data matrix, $\mathbf{A}$ is the ``mixing'' matrix, and $\mathbf{s}$ is the source matrix.  In the nomenclature we are using, $\mathbf{A}$ corresponds to the loading matrix $\mathbf{W}$, and $\mathbf{s}$ corresponds to the basis matrix $\mathbf{V}$.  In the ICA literature, the inverse of the mixing matrix $\mathbf{A}$ is denoted $\mathbf{W}$, but we will not be using that here to avoid confusion with the loading matrix.  Using our notation, the parallel problem to the PCA objective function (Equation \ref{eqn:reconstruction_error}) is something like: 

\begin{equation}
\begin{aligned}
& \underset{\mathbf{W}, \mathbf{V}}{\text{minimize}} & &  \| \mathbf{X} - \mathbf{WV}  \|_2^2 \\ 
&\text{subject to} & & \text{Columns of } \mathbf{V} \, \text{``independent.''}
\end{aligned}
\label{eqn:ica}
\end{equation}

In practice, though (at least as usually formulated), ICA uses a fundamentally different objective function from PCA. As explained before,  PCA maximizes the variance of the projections onto the eigenvectors.  This objective has a simple closed-form solution, but suffers from the drawback of assuming that the variance of the projections is a sufficient statistic to characterize the data.  If the data follows a Gaussian distribution, the variance is a sufficient statistic.  ICA, on the other hand, does not assume Gaussianity of the data.  Because of this, it cannot use variance as a statistic.  Instead, it first whitens the data so that it has unit variance, and then computes ``eigenvectors'' that give independent projections \cite{comon_independent_1994,hyvarinen_fast_1999,hyvarinen_independent_2000}.  The reason for whitening is that the distribution of Gaussian variables is invariant under orthogonal transformation \cite{comon_independent_1994}.  Because of this, using the variance of the dataset, which corresponds to second-order statistics, is only correct up to an orthogonal transformation.  Once the data has been whitened, second-order statistics are no longer useful for further transformations, and higher-order statistics are necessary.  Originally, kurtosis was used \cite{cardoso_source_1989}, but this has fallen out of favor and has been replaced by more robust approximations of non-Gaussianity. 

Therefore, instead of using the variance of the projections, ICA (at least as implemented in the FastICA algorithm, which is the most widely used algorithm for ICA \cite{hyvarinen_fast_1999,hyvarinen_independent_2000}) uses the negentropy, which is a measure of how far apart a given random variable is from Gaussianity: 
\begin{equation}
J_G \left( \mathbf{v}\right) = \left[ E \left\lbrace G \left( \mathbf{v}\transpose \mathbf{x}\right) \right\rbrace - E \left\lbrace G \left( \nu \right) \right\rbrace \right]^2,
\end{equation}
where $\mathbf{v}$ is a column of $\mathbf{V}$ and $\mathbf{x}$ is a row of $\mathbf{X}$,  $\nu$ is a Gaussian rv with unit variance (same as $\mathbf{v}$) and $G$ is some non-quadratic  ``contrast'' function, such as $\log \cosh (x)$. 

 \begin{equation}
\begin{aligned}
&\underset{\mathbf{v}}{\text{maximize}} && \sum_{i=1}^n J_G \left( \mathbf{v_i} \right) \\ 
& \text{subject to} & & \mathbf{V}\transpose\mathbf{V} = \mathbf{I}
\end{aligned}
\end{equation}

Note that in this part of the decomposition, there is no reconstruction cost term.  Because there is no dimensionality reduction at all in this stage and the transformation can be full rank, it is invertible and the original data can be recovered.  The reason for the rotation here is to produce highly kurtotic sources, not to reconstruct the data.   

\subsection*{Connection between SPCA and ICA}
This is intended as a sketch of the connection between SPCA and ICA.  If ICA is done in a one-step way, in which both orthogonality and kurtosis of the vectors is imposed (so that the PCA whitening stage and the ICA rotation stage) are performed at the same time, it will end up being very similar to SPCA.  This is because the kurtotic penalty on the ICA vectors makes the entries in the ICA basis vectors have a non-Gaussian distribution--spikier, with a heavier tail.  In practice, this corresponds to having many entries in the ICA basis vector be 0, and many have strongly non-0 values, the same idea as happens in an $\ell_1$-based sparseness penalty.  

\bibliographystyle{plain}
\bibliography{kandel_lib}

\end{document}